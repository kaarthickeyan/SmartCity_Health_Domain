{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a5d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. HELPER MODULES (GhostConv & CBAM)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class GhostConv(nn.Module):\n",
    "    \"\"\"\n",
    "    GhostConv: More Features from Cheap Operations.\n",
    "    Reduces computational cost by generating feature maps from cheap transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, relu=True):\n",
    "        super(GhostConv, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        init_channels = self.out_channels // 2\n",
    "        self.primary_conv = nn.Conv2d(in_channels, init_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.cheap_operation = nn.Conv2d(init_channels, init_channels, kernel_size=3, stride=1, padding=1, groups=init_channels, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True) if relu else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        primary_out = self.primary_conv(x)\n",
    "        cheap_out = self.cheap_operation(primary_out)\n",
    "        out = torch.cat([primary_out, cheap_out], dim=1)\n",
    "        return self.relu(out[:, :self.out_channels, :, :])\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\" Convolutional Block Attention Module to refine features. \"\"\"\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_attention(x)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. CORE ARCHITECTURAL BLOCKS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    \"\"\" A simplified MobileViT block for demonstration. \"\"\"\n",
    "    def __init__(self, in_channels, transformer_dim, ffn_dim, n_transformer_blocks, patch_size=(2, 2)):\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "        # Local representation\n",
    "        self.local_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        # Global representation (unfolding -> transformer -> folding)\n",
    "        self.global_conv1 = nn.Conv2d(in_channels, transformer_dim, kernel_size=1)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=4, dim_feedforward=ffn_dim, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_transformer_blocks)\n",
    "        self.global_conv2 = nn.Conv2d(transformer_dim, in_channels, kernel_size=1)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        local_features = self.local_conv(x)\n",
    "        \n",
    "        # Global path\n",
    "        y = self.global_conv1(x)\n",
    "        B, C, H, W = y.shape\n",
    "        y = y.unfold(2, self.patch_size[0], self.patch_size[0]).unfold(3, self.patch_size[1], self.patch_size[1])\n",
    "        y = y.contiguous().view(B, C, -1, self.patch_size[0] * self.patch_size[1])\n",
    "        y = y.permute(0, 2, 3, 1).contiguous().view(B, -1, C) # B, num_patches, channels\n",
    "        \n",
    "        y = self.transformer(y) # B, num_patches, channels\n",
    "        \n",
    "        y = y.view(B, H // self.patch_size[0], W // self.patch_size[1], C).permute(0, 3, 1, 2)\n",
    "        global_features = self.global_conv2(y)\n",
    "        \n",
    "        return local_features + global_features\n",
    "\n",
    "class BiFPNLayer(nn.Module):\n",
    "    \"\"\" Simplified BiFPN layer for efficient multi-scale feature fusion. \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(BiFPNLayer, self).__init__()\n",
    "        self.up_conv = GhostConv(channels, channels, kernel_size=3, padding=1)\n",
    "        self.down_conv = GhostConv(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # A true BiFPN has multiple levels, this is a simplified one-level example\n",
    "        P_high, P_low = inputs # Expects two feature maps of different scales\n",
    "        \n",
    "        # Top-down pathway\n",
    "        P_high_up = self.up_conv(P_high + F.interpolate(P_low, size=P_high.shape[-2:], mode='nearest'))\n",
    "        \n",
    "        # Bottom-up pathway\n",
    "        P_low_down = self.down_conv(P_low + F.max_pool2d(P_high_up, kernel_size=2, stride=2))\n",
    "        \n",
    "        return [P_high_up, P_low_down]\n",
    "\n",
    "class DecoupledHead(nn.Module):\n",
    "    \"\"\" Decoupled head for classification, regression, and objectness. \"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(DecoupledHead, self).__init__()\n",
    "        # Classification branch\n",
    "        self.cls_conv = nn.Sequential(\n",
    "            GhostConv(in_channels, in_channels, relu=True),\n",
    "            CBAM(in_channels),\n",
    "            nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "        )\n",
    "        # Regression branch\n",
    "        self.reg_conv = nn.Sequential(\n",
    "            GhostConv(in_channels, in_channels, relu=True),\n",
    "            CBAM(in_channels),\n",
    "            nn.Conv2d(in_channels, 4, kernel_size=1)  # 4 for bbox coords (x, y, w, h)\n",
    "        )\n",
    "        # Objectness branch\n",
    "        self.obj_conv = nn.Sequential(\n",
    "            GhostConv(in_channels, in_channels, relu=True),\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=1)  # 1 for objectness score\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cls_conv(x), self.reg_conv(x), self.obj_conv(x)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. THE COMPLETE CUSTOM MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CustomTrafficDetector(nn.Module):\n",
    "    def __init__(self, num_classes=4): # e.g., car, bus, truck, ambulance\n",
    "        super(CustomTrafficDetector, self).__init__()\n",
    "        # Backbone: A more complete backbone would have multiple stages\n",
    "        self.backbone_stage1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            MobileViTBlock(16, 32, 64, 2)\n",
    "        )\n",
    "        self.backbone_stage2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            MobileViTBlock(32, 64, 128, 2)\n",
    "        )\n",
    "        \n",
    "        # Neck\n",
    "        self.neck = BiFPNLayer(channels=32) # Simplified for two feature maps\n",
    "        \n",
    "        # Head\n",
    "        self.head = DecoupledHead(in_channels=32, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get multi-scale features from backbone\n",
    "        p1 = self.backbone_stage1(x)\n",
    "        p2 = self.backbone_stage2(p1)\n",
    "        \n",
    "        # Feature fusion in the neck\n",
    "        features = self.neck([p1, p2])\n",
    "        \n",
    "        # Detection head predicts on each feature map from the neck\n",
    "        # Here we only predict on the first one for simplicity\n",
    "        feature_map_for_detection = features[0]\n",
    "        cls_out, reg_out, obj_out = self.head(feature_map_for_detection)\n",
    "        \n",
    "        return cls_out, reg_out, obj_out\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. LOSS FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def bbox_ciou(box1, box2, eps=1e-7):\n",
    "    # Convert boxes from (center_x, center_y, w, h) to (x1, y1, x2, y2)\n",
    "    b1_x1, b1_y1 = box1[..., 0] - box1[..., 2] / 2, box1[..., 1] - box1[..., 3] / 2\n",
    "    b1_x2, b1_y2 = box1[..., 0] + box1[..., 2] / 2, box1[..., 1] + box1[..., 3] / 2\n",
    "    b2_x1, b2_y1 = box2[..., 0] - box2[..., 2] / 2, box2[..., 1] - box2[..., 3] / 2\n",
    "    b2_x2, b2_y2 = box2[..., 0] + box2[..., 2] / 2, box2[..., 1] + box2[..., 3] / 2\n",
    "\n",
    "    # Intersection area\n",
    "    inter_area = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "                 (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
    "    union_area = w1 * h1 + w2 * h2 - inter_area + eps\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    # Enclosing box\n",
    "    cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)\n",
    "    ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n",
    "    c2 = cw**2 + ch**2 + eps\n",
    "\n",
    "    # Center distance\n",
    "    rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n",
    "    \n",
    "    # Aspect ratio\n",
    "    v = (4 / (torch.pi**2)) * torch.pow(torch.atan(w2 / (h2 + eps)) - torch.atan(w1 / (h1 + eps)), 2)\n",
    "    with torch.no_grad():\n",
    "        alpha = v / (1 - iou + v + eps)\n",
    "    \n",
    "    return iou - (rho2 / c2 + v * alpha)\n",
    "\n",
    "class CIoULoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return (1.0 - bbox_ciou(pred, target)).mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class CompositeLoss(nn.Module):\n",
    "    def __init__(self, lambda_reg=1.0, lambda_cls=1.0, lambda_obj=1.0):\n",
    "        super(CompositeLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lambda_cls = lambda_cls\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.reg_loss = CIoULoss()\n",
    "        self.cls_loss = FocalLoss()\n",
    "        self.obj_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        cls_pred, reg_pred, obj_pred = preds\n",
    "        # Note: You will need to match predictions to targets\n",
    "        # This is a complex part of object detection (e.g., using anchor matching)\n",
    "        # For simplicity, we assume targets are already matched.\n",
    "        loss_reg = self.reg_loss(reg_pred[targets['mask']], targets['bbox'][targets['mask']])\n",
    "        loss_cls = self.cls_loss(cls_pred[targets['mask']], targets['cls'][targets['mask']])\n",
    "        loss_obj = self.obj_loss(obj_pred, targets['obj'])\n",
    "\n",
    "        total_loss = self.lambda_reg * loss_reg + self.lambda_cls * loss_cls + self.lambda_obj * loss_obj\n",
    "        return total_loss\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. USAGE EXAMPLE\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Ensure you have PyTorch installed: pip install torch torchvision\n",
    "    \n",
    "#     # --- Model Initialization ---\n",
    "#     num_classes = 4  # e.g., 0: car, 1: bus, 2: truck, 3: ambulance\n",
    "#     model = CustomTrafficDetector(num_classes=num_classes)\n",
    "#     print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters.\")\n",
    "    \n",
    "#     # --- Dummy Input ---\n",
    "#     batch_size = 2\n",
    "#     input_image = torch.randn(batch_size, 3, 256, 256) # B, C, H, W\n",
    "    \n",
    "#     # --- Forward Pass ---\n",
    "#     cls_out, reg_out, obj_out = model(input_image)\n",
    "#     print(\"Output shapes:\")\n",
    "#     print(f\"  Classification: {cls_out.shape}\")\n",
    "#     print(f\"  Regression:     {reg_out.shape}\")\n",
    "#     print(f\"  Objectness:     {obj_out.shape}\")\n",
    "\n",
    "#     # --- Loss Calculation Example ---\n",
    "#     loss_fn = CompositeLoss()\n",
    "    \n",
    "#     # Dummy targets (in a real scenario, these come from your dataloader)\n",
    "#     # The 'mask' indicates which grid cells contain an object.\n",
    "#     mask = torch.zeros(cls_out.shape[0], cls_out.shape[2], cls_out.shape[3], dtype=torch.bool)\n",
    "#     mask[0, 10, 10] = True # Example: one object in the first image\n",
    "    \n",
    "#     targets = {\n",
    "#         'bbox': torch.randn(mask.sum(), 4), # Dummy bbox values for the object\n",
    "#         'cls': torch.zeros(mask.sum(), num_classes).scatter_(1, torch.randint(0, num_classes, (mask.sum(), 1)), 1.), # One-hot class\n",
    "#         'obj': mask.float().unsqueeze(1), # Objectness target\n",
    "#         'mask': mask\n",
    "#     }\n",
    "\n",
    "#     # You'll need to process the model output to match the target format\n",
    "#     # This is a simplification.\n",
    "#     # total_loss = loss_fn((cls_out, reg_out, obj_out), targets)\n",
    "#     # print(f\"\\nExample Loss (requires proper target matching): {total_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e3420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
